# Model Architecture Configuration
model:
  # Model size configuration (500M parameters target)
  n_layers: 12              # Number of transformer layers
  n_heads: 12               # Number of attention heads  
  n_embd: 768               # Hidden dimension size
  vocab_size: 50304         # Vocabulary size (divisible by 64 for efficiency)
  block_size: 1024          # Context length / sequence length
  
  # Architecture options
  dropout: 0.1              # Dropout probability
  bias: true                # Use bias in linear layers
  
  # Advanced options
  use_flash_attention: false # Use FlashAttention if available
  gradient_checkpointing: false # Enable gradient checkpointing to save memory
  
  # Initialization
  initializer_range: 0.02   # Standard deviation for weight initialization
  
# Model variants - uncomment to use different sizes
# Small model (125M parameters)
# model:
#   n_layers: 12
#   n_heads: 12  
#   n_embd: 768
#   vocab_size: 50304
#   block_size: 1024

# Medium model (350M parameters)  
# model:
#   n_layers: 24
#   n_heads: 16
#   n_embd: 1024
#   vocab_size: 50304
#   block_size: 1024

# Large model (774M parameters)
# model:
#   n_layers: 24
#   n_heads: 16
#   n_embd: 1536  
#   vocab_size: 50304
#   block_size: 1024